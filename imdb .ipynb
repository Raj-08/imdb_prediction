{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                      movie_title             color  \\\n",
       "0                                 Stuart Little              Color   \n",
       "1                                         Crash              Color   \n",
       "2              Ghost Rider: Spirit of Vengeance              Color   \n",
       "3                                    Gettysburg              Color   \n",
       "4                            Planet of the Apes              Color   \n",
       "5                         Daredevil                            NaN   \n",
       "6                      The Pursuit of Happyness              Color   \n",
       "7                           Kiss Kiss Bang Bang              Color   \n",
       "8                              Music and Lyrics              Color   \n",
       "9                               Horrible Bosses              Color   \n",
       "10                                    Blindness              Color   \n",
       "11                                    Cleopatra              Color   \n",
       "12                                  Out of Time              Color   \n",
       "13                        My Best Friend's Girl              Color   \n",
       "14                             Ruby in Paradise              Color   \n",
       "15                                    The Quiet              Color   \n",
       "16              The Powerpuff Girls                          Color   \n",
       "17                               Black November              Color   \n",
       "18             Dodgeball: A True Underdog Story    Black and White   \n",
       "19                          Grand Theft Parsons              Color   \n",
       "20                                    Losin' It              Color   \n",
       "21                                   Royal Kill              Color   \n",
       "22                                     Steamboy              Color   \n",
       "23                        The French Connection              Color   \n",
       "24               The Return of the Pink Panther              Color   \n",
       "25                         Flags of Our Fathers              Color   \n",
       "26                    El crimen del padre Amaro              Color   \n",
       "27                     The Secret in Their Eyes              Color   \n",
       "28                               Reservoir Dogs              Color   \n",
       "29                                  Chicken Run              Color   \n",
       "...                                          ...               ...   \n",
       "4270                                 Life of Pi              Color   \n",
       "4271                                    Frailty              Color   \n",
       "4272                        From Dusk Till Dawn              Color   \n",
       "4273                                Bright Star              Color   \n",
       "4274                          Final Destination              Color   \n",
       "4275                                 Changeling              Color   \n",
       "4276                         The Banger Sisters              Color   \n",
       "4277                                 Battleship              Color   \n",
       "4278                            Poltergeist III              Color   \n",
       "4279                         A Walk to Remember              Color   \n",
       "4280                                       Bang              Color   \n",
       "4281                    My Beautiful Laundrette              Color   \n",
       "4282                     300: Rise of an Empire              Color   \n",
       "4283                                    Bronson              Color   \n",
       "4284                          Hollywood Shuffle              Color   \n",
       "4285                        The Blood of Heroes              Color   \n",
       "4286                      House of 1000 Corpses    Black and White   \n",
       "4287                   Alexander's Ragtime Band    Black and White   \n",
       "4288                              Enter Nowhere              Color   \n",
       "4289                   Clear and Present Danger              Color   \n",
       "4290                           Agent Cody Banks              Color   \n",
       "4291                                Buffalo '66              Color   \n",
       "4292                              Anna Karenina              Color   \n",
       "4293                               Pandaemonium              Color   \n",
       "4294                         Out of the Furnace              Color   \n",
       "4295  The Twilight Saga: Breaking Dawn - Part 2              Color   \n",
       "4296             Diary of a Wimpy Kid: Dog Days              Color   \n",
       "4297             The Doombolt Chase                          Color   \n",
       "4298                                  Treachery              Color   \n",
       "4299                                    Get Low              Color   \n",
       "\n",
       "                director_name  num_critic_for_reviews  duration  \\\n",
       "0                 Rob Minkoff                   101.0      84.0   \n",
       "1                 Paul Haggis                   287.0     115.0   \n",
       "2              Mark Neveldine                   287.0      96.0   \n",
       "3                 Ron Maxwell                    22.0     271.0   \n",
       "4                  Tim Burton                   230.0     119.0   \n",
       "5                         NaN                    95.0      54.0   \n",
       "6            Gabriele Muccino                   201.0     117.0   \n",
       "7                 Shane Black                   223.0     103.0   \n",
       "8               Marc Lawrence                   175.0      95.0   \n",
       "9                 Seth Gordon                   340.0     106.0   \n",
       "10         Fernando Meirelles                   229.0     121.0   \n",
       "11       Joseph L. Mankiewicz                    72.0     251.0   \n",
       "12              Carl Franklin                   124.0     114.0   \n",
       "13              Howard Deutch                    80.0     112.0   \n",
       "14               Victor Nunez                    11.0     114.0   \n",
       "15               Jamie Babbit                    65.0      91.0   \n",
       "16                        NaN                     7.0      30.0   \n",
       "17                 Jeta Amata                     6.0      95.0   \n",
       "18    Rawson Marshall Thurber                   191.0      92.0   \n",
       "19              David Caffrey                    51.0      88.0   \n",
       "20              Curtis Hanson                     7.0     100.0   \n",
       "21                Babar Ahmed                     8.0      90.0   \n",
       "22            Katsuhiro Ôtomo                   105.0     103.0   \n",
       "23           William Friedkin                   138.0     104.0   \n",
       "24              Blake Edwards                    33.0     113.0   \n",
       "25             Clint Eastwood                   279.0     135.0   \n",
       "26             Carlos Carrera                    85.0     118.0   \n",
       "27       Juan José Campanella                   262.0     129.0   \n",
       "28          Quentin Tarantino                   173.0      99.0   \n",
       "29                 Peter Lord                   187.0      84.0   \n",
       "...                       ...                     ...       ...   \n",
       "4270                  Ang Lee                   552.0     127.0   \n",
       "4271              Bill Paxton                   161.0     100.0   \n",
       "4272         Robert Rodriguez                   165.0     108.0   \n",
       "4273             Jane Campion                   222.0     119.0   \n",
       "4274               James Wong                   211.0      98.0   \n",
       "4275           Clint Eastwood                   264.0     141.0   \n",
       "4276               Bob Dolman                    92.0      98.0   \n",
       "4277               Peter Berg                   377.0     131.0   \n",
       "4278             Gary Sherman                    66.0      98.0   \n",
       "4279            Adam Shankman                    80.0     101.0   \n",
       "4280          Ash Baron-Cohen                    10.0      98.0   \n",
       "4281           Stephen Frears                    46.0      97.0   \n",
       "4282               Noam Murro                   366.0     102.0   \n",
       "4283     Nicolas Winding Refn                   225.0      92.0   \n",
       "4284          Robert Townsend                    21.0      81.0   \n",
       "4285       David Webb Peoples                    21.0      90.0   \n",
       "4286               Rob Zombie                   202.0     105.0   \n",
       "4287               Henry King                    10.0     106.0   \n",
       "4288              Jack Heller                    26.0      90.0   \n",
       "4289            Phillip Noyce                    42.0     141.0   \n",
       "4290             Harald Zwart                    79.0     102.0   \n",
       "4291            Vincent Gallo                   106.0     110.0   \n",
       "4292               Joe Wright                   324.0     129.0   \n",
       "4293            Julien Temple                    22.0     124.0   \n",
       "4294             Scott Cooper                   288.0     116.0   \n",
       "4295              Bill Condon                   322.0     115.0   \n",
       "4296             David Bowers                    88.0      94.0   \n",
       "4297                      NaN                     NaN      30.0   \n",
       "4298            Travis Romero                     5.0      67.0   \n",
       "4299          Aaron Schneider                   160.0     100.0   \n",
       "\n",
       "      director_facebook_likes  actor_3_facebook_likes  \\\n",
       "0                        50.0                   692.0   \n",
       "1                       549.0                   911.0   \n",
       "2                        83.0                  1000.0   \n",
       "3                        33.0                   251.0   \n",
       "4                     13000.0                   567.0   \n",
       "5                         NaN                     0.0   \n",
       "6                       125.0                   168.0   \n",
       "7                      1000.0                   611.0   \n",
       "8                        30.0                   664.0   \n",
       "9                        91.0                    64.0   \n",
       "10                      353.0                    18.0   \n",
       "11                      311.0                   595.0   \n",
       "12                       73.0                   323.0   \n",
       "13                       41.0                   490.0   \n",
       "14                        9.0                    86.0   \n",
       "15                       91.0                   659.0   \n",
       "16                        NaN                   265.0   \n",
       "17                       20.0                    36.0   \n",
       "18                       43.0                   936.0   \n",
       "19                        4.0                    95.0   \n",
       "20                      161.0                   172.0   \n",
       "21                        0.0                    32.0   \n",
       "22                       78.0                   101.0   \n",
       "23                      607.0                   109.0   \n",
       "24                      688.0                    80.0   \n",
       "25                    16000.0                   310.0   \n",
       "26                       12.0                    67.0   \n",
       "27                      195.0                    50.0   \n",
       "28                    16000.0                   455.0   \n",
       "29                       91.0                   206.0   \n",
       "...                       ...                     ...   \n",
       "4270                      0.0                   341.0   \n",
       "4271                      0.0                   303.0   \n",
       "4272                      0.0                   844.0   \n",
       "4273                    319.0                   179.0   \n",
       "4274                     70.0                   591.0   \n",
       "4275                  16000.0                   539.0   \n",
       "4276                      7.0                    73.0   \n",
       "4277                    532.0                   627.0   \n",
       "4278                     17.0                   770.0   \n",
       "4279                    163.0                   488.0   \n",
       "4280                      3.0                   152.0   \n",
       "4281                    350.0                    33.0   \n",
       "4282                    263.0                  1000.0   \n",
       "4283                      0.0                   146.0   \n",
       "4284                    467.0                   287.0   \n",
       "4285                     73.0                    26.0   \n",
       "4286                      0.0                   303.0   \n",
       "4287                     85.0                   300.0   \n",
       "4288                      0.0                   120.0   \n",
       "4289                    176.0                   672.0   \n",
       "4290                     91.0                   434.0   \n",
       "4291                    787.0                   642.0   \n",
       "4292                    456.0                     0.0   \n",
       "4293                     67.0                   303.0   \n",
       "4294                    108.0                   120.0   \n",
       "4295                    386.0                 12000.0   \n",
       "4296                     42.0                   517.0   \n",
       "4297                      NaN                     9.0   \n",
       "4298                      4.0                  2000.0   \n",
       "4299                     11.0                   970.0   \n",
       "\n",
       "                 actor_2_name  actor_1_facebook_likes        gross    ...      \\\n",
       "0                 Nathan Lane                   979.0  140015224.0    ...       \n",
       "1              Loretta Devine                  3000.0   54557348.0    ...       \n",
       "2             Spencer Wilding                 12000.0   51774002.0    ...       \n",
       "3     William Morgan Sheppard                   854.0   10769960.0    ...       \n",
       "4              Estella Warren                  1000.0  180011740.0    ...       \n",
       "5               Royce Johnson                   577.0          NaN    ...       \n",
       "6                 Kurt Fuller                 10000.0  162586036.0    ...       \n",
       "7              Corbin Bernsen                 21000.0    4235837.0    ...       \n",
       "8                Scott Porter                   799.0   50562555.0    ...       \n",
       "9              Lindsay Sloane                 18000.0  117528646.0    ...       \n",
       "10                 Joe Pingue                    45.0    3073392.0    ...       \n",
       "11             Richard Burton                   940.0   57750000.0    ...       \n",
       "12               Sanaa Lathan                 18000.0   40905277.0    ...       \n",
       "13               Taran Killam                  1000.0   19151864.0    ...       \n",
       "14                 Todd Field                   159.0    1001437.0    ...       \n",
       "15            David Gallagher                   982.0     381186.0    ...       \n",
       "16              Jennifer Hale                   971.0          NaN    ...       \n",
       "17              Nathin Butler                   262.0          NaN    ...       \n",
       "18               Stephen Root                   989.0  114324072.0    ...       \n",
       "19                Scott Adsit                   690.0          NaN    ...       \n",
       "20               Shelley Long                 10000.0          NaN    ...       \n",
       "21           Alexander Wraith                   502.0          NaN    ...       \n",
       "22         Robin Atkin Downes                   488.0     410388.0    ...       \n",
       "23               Fernando Rey                   813.0          NaN    ...       \n",
       "24                Herbert Lom                   462.0          NaN    ...       \n",
       "25                Chris Bauer                 23000.0   33574332.0    ...       \n",
       "26       Ana Claudia Talancón                   201.0    5709616.0    ...       \n",
       "27           Soledad Villamil                   827.0   20167424.0    ...       \n",
       "28              Steve Buscemi                 16000.0    2812029.0    ...       \n",
       "29         Miranda Richardson                   579.0  106793915.0    ...       \n",
       "...                       ...                     ...          ...    ...       \n",
       "4270               Rafe Spall                   774.0  124976634.0    ...       \n",
       "4271            Powers Boothe                 11000.0   13103828.0    ...       \n",
       "4272              Salma Hayek                 16000.0   25753840.0    ...       \n",
       "4273           Paul Schneider                  2000.0    4440055.0    ...       \n",
       "4274             Brendan Fehr                  1000.0   53302314.0    ...       \n",
       "4275            Michael Kelly                 11000.0   35707327.0    ...       \n",
       "4276       Eva Amurri Martino                   931.0   30306281.0    ...       \n",
       "4277      Alexander Skarsgård                 14000.0   65173160.0    ...       \n",
       "4278         Heather O'Rourke                  1000.0   14114488.0    ...       \n",
       "4279             Peter Coyote                   683.0   41227069.0    ...       \n",
       "4280        Stanley B. Herman                   789.0          NaN    ...       \n",
       "4281              Roshan Seth                   114.0          NaN    ...       \n",
       "4282       Sullivan Stapleton                  6000.0  106369117.0    ...       \n",
       "4283              James Lance                 27000.0     104792.0    ...       \n",
       "4284      Keenen Ivory Wayans                   467.0    5228617.0    ...       \n",
       "4285                Joan Chen                   848.0     882290.0    ...       \n",
       "4286          Matthew McGrory                  1000.0   12583510.0    ...       \n",
       "4287               Don Ameche                   480.0          NaN    ...       \n",
       "4288      Katherine Waterston                   322.0          NaN    ...       \n",
       "4289               Dean Jones                 11000.0  122012710.0    ...       \n",
       "4290            Frankie Muniz                  1000.0   47285499.0    ...       \n",
       "4291            Vincent Gallo                  1000.0    2365931.0    ...       \n",
       "4292      Guro Nagelhus Schia                  2000.0   12802907.0    ...       \n",
       "4293          Dexter Fletcher                   631.0          NaN    ...       \n",
       "4294              Sam Shepard                 23000.0   11326836.0    ...       \n",
       "4295          Kristen Stewart                 21000.0  292298923.0    ...       \n",
       "4296           Rachael Harris                   975.0   49002815.0    ...       \n",
       "4297         George Coulouris                   310.0          NaN    ...       \n",
       "4298            Lorraine Ziff                260000.0          NaN    ...       \n",
       "4299            Robert Duvall                 13000.0    9176553.0    ...       \n",
       "\n",
       "     num_user_for_reviews  language    country  content_rating        budget  \\\n",
       "0                   179.0   English    Germany              PG  1.330000e+08   \n",
       "1                  1624.0   English    Germany               R  6.500000e+06   \n",
       "2                   331.0   English        USA           PG-13  5.700000e+07   \n",
       "3                   256.0   English        USA              PG  2.500000e+07   \n",
       "4                  1368.0   English        USA           PG-13  1.000000e+08   \n",
       "5                   394.0   English        USA           TV-MA           NaN   \n",
       "6                   611.0   English        USA           PG-13  5.500000e+07   \n",
       "7                   336.0   English        USA               R  1.500000e+07   \n",
       "8                   291.0   English        USA           PG-13           NaN   \n",
       "9                   387.0   English        USA               R  3.500000e+07   \n",
       "10                  300.0   English     Canada               R  2.500000e+07   \n",
       "11                  192.0   English         UK        Approved  3.111500e+07   \n",
       "12                  157.0   English        USA               R  5.000000e+07   \n",
       "13                  100.0   English        USA               R  4.000000e+07   \n",
       "14                   28.0   English        USA               R  8.000000e+05   \n",
       "15                   90.0   English        USA               R  9.000000e+05   \n",
       "16                   60.0   English        USA           TV-Y7           NaN   \n",
       "17                    4.0   English    Nigeria             NaN  7.500000e+06   \n",
       "18                  392.0   English        USA           PG-13  2.000000e+07   \n",
       "19                   38.0   English        USA           PG-13           NaN   \n",
       "20                   16.0   English     Canada               R  7.000000e+06   \n",
       "21                   18.0   English        USA           PG-13  3.500000e+05   \n",
       "22                   79.0  Japanese      Japan           PG-13  2.127520e+09   \n",
       "23                  280.0   English        USA               R  1.800000e+06   \n",
       "24                   73.0   English         UK               G  5.000000e+06   \n",
       "25                  415.0   English        USA               R  9.000000e+07   \n",
       "26                  110.0   Spanish     Mexico               R  1.800000e+06   \n",
       "27                  231.0   Spanish  Argentina               R  2.000000e+06   \n",
       "28                  931.0   English        USA               R  1.200000e+06   \n",
       "29                  358.0   English         UK               G  4.500000e+07   \n",
       "...                   ...       ...        ...             ...           ...   \n",
       "4270                755.0   English        USA              PG  1.200000e+08   \n",
       "4271                463.0   English        USA               R  1.100000e+07   \n",
       "4272                592.0   English        USA               R  1.900000e+07   \n",
       "4273                110.0   English         UK              PG  8.500000e+06   \n",
       "4274                674.0   English        USA               R  2.300000e+07   \n",
       "4275                387.0   English        USA               R  5.500000e+07   \n",
       "4276                147.0   English        USA               R  1.000000e+07   \n",
       "4277                751.0   English        USA           PG-13  2.090000e+08   \n",
       "4278                114.0   English        USA           PG-13  1.050000e+07   \n",
       "4279                962.0   English        USA              PG  1.100000e+07   \n",
       "4280                 14.0   English        USA             NaN           NaN   \n",
       "4281                 64.0   English         UK               R  6.500000e+05   \n",
       "4282                523.0   English        USA               R  1.100000e+08   \n",
       "4283                145.0   English         UK               R           NaN   \n",
       "4284                 32.0   English        USA               R  1.000000e+05   \n",
       "4285                 43.0   English  Australia               R  1.000000e+07   \n",
       "4286                922.0   English        USA               R  7.000000e+06   \n",
       "4287                 29.0   English        USA        Approved  2.000000e+06   \n",
       "4288                 45.0   English        USA               R  5.000000e+05   \n",
       "4289                133.0   English        USA           PG-13  6.200000e+07   \n",
       "4290                104.0   English        USA              PG  2.600000e+07   \n",
       "4291                318.0   English        USA               R  1.500000e+06   \n",
       "4292                226.0   English         UK               R           NaN   \n",
       "4293                 32.0   English         UK           PG-13  1.500000e+07   \n",
       "4294                231.0   English        USA               R  2.200000e+07   \n",
       "4295                329.0   English        USA           PG-13  1.200000e+08   \n",
       "4296                 35.0   English        USA              PG  2.200000e+07   \n",
       "4297                  NaN   English         UK             NaN           NaN   \n",
       "4298                  5.0   English        USA             NaN  6.250000e+05   \n",
       "4299                 97.0   English        USA           PG-13  7.500000e+06   \n",
       "\n",
       "      title_year actor_2_facebook_likes aspect_ratio  movie_facebook_likes  \\\n",
       "0         1999.0                  886.0         1.85                     0   \n",
       "1         2004.0                  912.0         2.35                 18000   \n",
       "2         2011.0                 1000.0         2.35                 18000   \n",
       "3         1993.0                  702.0         1.85                     0   \n",
       "4         2001.0                  658.0         2.35                     0   \n",
       "5            NaN                    4.0        16.00                 55000   \n",
       "6         2006.0                  617.0         2.35                 32000   \n",
       "7         2005.0                 1000.0         2.35                     0   \n",
       "8         2007.0                  690.0         1.85                     0   \n",
       "9         2011.0                  464.0         2.35                 31000   \n",
       "10        2008.0                   30.0         1.85                     0   \n",
       "11        1963.0                  726.0         2.20                     0   \n",
       "12        2003.0                  886.0         2.35                  1000   \n",
       "13        2008.0                  500.0         1.85                     0   \n",
       "14        1993.0                  143.0         1.85                    81   \n",
       "15        2005.0                  796.0         2.35                   698   \n",
       "16           NaN                  918.0         4.00                   581   \n",
       "17        2012.0                   65.0          NaN                   389   \n",
       "18        2004.0                  939.0         2.35                     0   \n",
       "19        2003.0                  316.0         1.85                   480   \n",
       "20        1983.0                  422.0          NaN                   168   \n",
       "21        2009.0                  119.0          NaN                    53   \n",
       "22        2004.0                  336.0         1.85                   973   \n",
       "23        1971.0                  165.0         1.85                     0   \n",
       "24        1975.0                  278.0         2.35                   620   \n",
       "25        2006.0                  638.0         2.35                     0   \n",
       "26        2002.0                  163.0         1.85                   544   \n",
       "27        2009.0                   88.0         2.35                 33000   \n",
       "28        1992.0                12000.0         2.35                 19000   \n",
       "29        2000.0                  530.0         1.85                     0   \n",
       "...          ...                    ...          ...                   ...   \n",
       "4270      2012.0                  358.0         1.85                122000   \n",
       "4271      2001.0                  472.0         1.85                  5000   \n",
       "4272      1996.0                 4000.0         1.85                 12000   \n",
       "4273      2009.0                  552.0         1.85                     0   \n",
       "4274      2000.0                  847.0         1.85                     0   \n",
       "4275      2008.0                  963.0         2.35                 14000   \n",
       "4276      2002.0                  797.0         2.35                   744   \n",
       "4277      2012.0                10000.0         2.35                 44000   \n",
       "4278      1988.0                  887.0         1.85                   616   \n",
       "4279      2002.0                  548.0         2.35                 19000   \n",
       "4280      1995.0                  194.0          NaN                    20   \n",
       "4281      1985.0                   61.0         1.66                     0   \n",
       "4282      2014.0                 1000.0         2.35                 71000   \n",
       "4283      2008.0                  161.0         1.85                 22000   \n",
       "4284      1987.0                  322.0         1.85                   471   \n",
       "4285      1989.0                  643.0         1.33                   999   \n",
       "4286      2003.0                  340.0         1.85                     0   \n",
       "4287      1938.0                  392.0         1.37                    60   \n",
       "4288      2011.0                  178.0          NaN                   701   \n",
       "4289      1994.0                  913.0         2.35                     0   \n",
       "4290      2003.0                  934.0         2.35                   542   \n",
       "4291      1998.0                  787.0         1.85                     0   \n",
       "4292      2012.0                   35.0         2.35                 36000   \n",
       "4293      2000.0                  452.0          NaN                   277   \n",
       "4294      2013.0                  820.0         2.35                 17000   \n",
       "4295      2012.0                17000.0         2.35                 65000   \n",
       "4296      2012.0                  569.0         2.35                     0   \n",
       "4297         NaN                   11.0          NaN                     0   \n",
       "4298      2013.0                21000.0          NaN                     0   \n",
       "4299      2009.0                 3000.0         2.35                     0   \n",
       "\n",
       "     imdb_score  \n",
       "0           5.9  \n",
       "1           7.9  \n",
       "2           4.3  \n",
       "3           7.7  \n",
       "4           5.7  \n",
       "5           8.8  \n",
       "6           8.0  \n",
       "7           7.6  \n",
       "8           6.5  \n",
       "9           6.9  \n",
       "10          6.6  \n",
       "11          7.0  \n",
       "12          6.5  \n",
       "13          5.9  \n",
       "14          7.2  \n",
       "15          6.4  \n",
       "16          7.2  \n",
       "17          5.6  \n",
       "18          6.7  \n",
       "19          6.1  \n",
       "20          4.8  \n",
       "21          3.2  \n",
       "22          6.9  \n",
       "23          7.8  \n",
       "24          7.1  \n",
       "25          7.1  \n",
       "26          6.8  \n",
       "27          8.2  \n",
       "28          8.4  \n",
       "29          7.0  \n",
       "...         ...  \n",
       "4270        8.0  \n",
       "4271        7.3  \n",
       "4272        7.3  \n",
       "4273        7.0  \n",
       "4274        6.7  \n",
       "4275        7.8  \n",
       "4276        5.6  \n",
       "4277        5.9  \n",
       "4278        4.5  \n",
       "4279        7.4  \n",
       "4280        6.4  \n",
       "4281        6.9  \n",
       "4282        6.2  \n",
       "4283        7.1  \n",
       "4284        7.0  \n",
       "4285        6.5  \n",
       "4286        6.0  \n",
       "4287        7.0  \n",
       "4288        6.6  \n",
       "4289        6.9  \n",
       "4290        5.0  \n",
       "4291        7.5  \n",
       "4292        6.6  \n",
       "4293        6.6  \n",
       "4294        6.8  \n",
       "4295        5.5  \n",
       "4296        6.4  \n",
       "4297        7.2  \n",
       "4298        3.9  \n",
       "4299        7.1  \n",
       "\n",
       "[4300 rows x 28 columns]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from libs import utils\n",
    "#import dataframe as df\n",
    "\n",
    "\n",
    "movie = pd.read_csv('movie_data_for_assignment.csv')\n",
    "movie.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "str_list = [] # Removing coloumns with strings\n",
    "for colname, colvalue in movie.iteritems():\n",
    "    if type(colvalue[1]) == str:\n",
    "         str_list.append(colname)\n",
    "            \n",
    "num_list = movie.columns.difference(str_list)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4300, 16)\n",
      "[ 5.9]\n",
      "(4300, 1)\n",
      "(4300, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "movie_num = movie[num_list]\n",
    "# # Extracting 'imdb_score' from dataset and deleting the coloumn from datase\n",
    "#movie_num.head()\n",
    "numpyMatrix = movie_num.as_matrix()\n",
    "print(numpyMatrix.shape)\n",
    "x_std= numpyMatrix[:, [1,9]]\n",
    "y_std = numpyMatrix[:, [10]]\n",
    "testMoviey=y_std[4]\n",
    "testMoviex=x_std[4]\n",
    "print(y_std[0])\n",
    "print(y_std.shape)\n",
    "nump1 = np.delete(numpyMatrix, [10], axis=1)\n",
    "movie_num.shape;\n",
    "print(nump1.shape)\n",
    "movie_num1=movie_num.drop('imdb_score', axis=1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_num = movie_num.fillna(value=0, axis=1) #replacing nan values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.9]\n",
      " [ 7.9]\n",
      " [ 4.3]\n",
      " ..., \n",
      " [ 7.2]\n",
      " [ 3.9]\n",
      " [ 7.1]]\n"
     ]
    }
   ],
   "source": [
    "X = movie_num.values # Normalizing our data using sklearn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "#y_std=X_std[:, [15]]\n",
    "print(y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort from high to low\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "# Calculation of Explained Variance from the eigenvalues\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] \n",
    "cum_var_exp = np.cumsum(var_exp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3) #Running PCA on our normalized data to reduce dimensionality from 15 to 4\n",
    "x_std = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys = y_std\n",
    "xs = x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4300, 3)\n",
      "(4300, 1)\n"
     ]
    }
   ],
   "source": [
    "print(xs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First create a tensorflow session\n",
    "sess = tf.Session();\n",
    "\n",
    "# Now create an operation that will calculate the mean of our images\n",
    "mean_xs = tf.reduce_mean(xs, axis=[1], keep_dims=False, name=None, reduction_indices=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_xs = sess.run(mean_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4300,)\n",
      "[[-0.22634565]\n",
      " [ 0.03644536]\n",
      " [ 0.19902708]\n",
      " ..., \n",
      " [-2.02340597]\n",
      " [ 9.47658805]\n",
      " [ 0.26645763]]\n"
     ]
    }
   ],
   "source": [
    "print(mean_xs.shape)\n",
    "mean_xs = np.reshape(mean_xs,(4300,1))\n",
    "print(mean_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "mean_xs_4d = tf.reduce_mean(xs, reduction_indices=0, keep_dims=True);\n",
    "print(mean_xs_4d.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subtraction = xs - mean_xs_4d\n",
    "\n",
    "# Now computing the standard deviation by calculating the\n",
    "# square root of the expected squared differences\n",
    "std_img_op = tf.sqrt(tf.reduce_mean(subtraction * subtraction, reduction_indices=0))\n",
    "# Now calculate the standard deviation using your session\n",
    "std_img = sess.run(std_img_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalising : by substracting from mean and then dividing by standard devitaion\n",
    "xs = tf.reshape(xs,[4300,3])\n",
    "norm_imgs_op = (xs-mean_xs)\n",
    "norm_imgs_op = norm_imgs_op/std_img;\n",
    "sess.run(tf.global_variables_initializer());\n",
    "norm_imgs = sess.run(norm_imgs_op);\n",
    "xs = norm_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4300, 10)\n"
     ]
    }
   ],
   "source": [
    "#Converting our imdb values(ys) into one hot encodings of shape (4300,10)\n",
    "ys=np.asarray(ys)\n",
    "one_hot_y=np.zeros((4300,10), dtype=float, order='C');\n",
    "import math\n",
    "for r in range(4300):\n",
    "    ys[r]= math.ceil(ys[r])\n",
    "for r in range(ys.shape[0]):\n",
    "    i=ys[r];\n",
    "    i=i[0].astype(int)\n",
    "    #i=tf.to_int32(i[0])\n",
    "   # print(i)\n",
    "    yz=np.zeros((1,10), dtype=float, order='C');\n",
    "    yz[0,i-1]=1\n",
    "   # np.append(yz,axis=0)\n",
    "    np.vstack((one_hot_y,yz))\n",
    "print(one_hot_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting our dataset into training,validation and test set\n",
    "x_train = xs[range(2000),:]\n",
    "y_train = ys[range(2000),:]\n",
    "x_valid = xs[range(2000,3000),:]\n",
    "y_valid = one_hot_y[range(2000,3000),:]\n",
    "x_test = xs[range(3000,4300),:]\n",
    "y_test = one_hot_y[range(3000,4300),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating a computational graph on Tensorflow\n",
    "X = tf.placeholder(tf.float32,name=\"X\",shape=[None,3]);\n",
    "W = tf.Variable(tf.random_normal([3,5],dtype=tf.float32,stddev = 0.1,name =\"W\"))\n",
    "h = tf.matmul(X,W);\n",
    "b = tf.Variable(tf.constant([0,1],dtype=tf.float32,shape=[5],name = \"b\"));\n",
    "h = tf.nn.bias_add(h,b);\n",
    "h = tf.nn.relu(h);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Linear function that creates a fully connected layer\n",
    "def linear(x, n_output, name=None, activation=None, reuse=None):\n",
    "    \n",
    "    if len(x.get_shape()) != 2:\n",
    "        x = flatten(x, reuse=reuse)\n",
    "\n",
    "    n_input = x.get_shape().as_list()[1]\n",
    "\n",
    "    with tf.variable_scope(name or \"fc\", reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='W',\n",
    "            shape=[n_input, n_output],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        b = tf.get_variable(\n",
    "            name='b',\n",
    "            shape=[n_output],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        h = tf.nn.bias_add(\n",
    "            name='h',\n",
    "            value=tf.matmul(x, W),\n",
    "            bias=b)\n",
    "\n",
    "       # h = tf.ceil(h,name=None)\n",
    "\n",
    "        \n",
    "        if activation:\n",
    "            h = activation(h)\n",
    "            \n",
    "        return h, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a placeholder of None x 4 dimensions and dtype tf.float32\n",
    "\n",
    "X = tf.placeholder(tf.float32,name=\"X\",shape=[None,3]);\n",
    "Y = tf.placeholder(tf.float32,name=\"Y\",shape=[None,1]);\n",
    "labels = tf.placeholder(tf.float32,name=\"labels\",shape=[None,10]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll create 7 hidden layers.  Let's create a variable to say how many neurons we want for each of the layers\n",
    "h1, W1 = linear(\n",
    "    x=X, n_output=10, name='linear1', activation=tf.nn.relu)\n",
    "h2, W2 = linear(\n",
    "    x=h1, n_output=10, name='linear2', activation=tf.nn.relu)\n",
    "h3, W3 =  linear(\n",
    "    x=h2, n_output=10, name='linear3', activation=tf.nn.relu)\n",
    "Y_pred, W4 =  linear(\n",
    "    x=h3, n_output=1, name='linear4', activation=None)\n",
    "#h5, W5 =  linear(\n",
    "   # x=h4, n_output=5, name='linear5', activation=tf.nn.relu)\n",
    "#h6,5 W6 =  linear(\n",
    "   # x=h5, n_output=5, name='linear6', activation=tf.nn.relu)\n",
    "#h7, W7 =  linear(\n",
    " #   x=h6, n_output=5, name='linear7', activation=tf.nn.relu)\n",
    "#keep_prob = tf.placeholder(tf.float32)\n",
    "#h_drop = tf.nn.dropout(h5, keep_prob)\n",
    "#Y_pred, W8 = linear(h1, 10, activation=None, name='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cost = tf.nn.sigmoid_cross_entropy_with_logits(Y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a session and executing out graph by feeding data in mini batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with tf.Session() as sess:\n",
    "    #cost = -tf.reduce_sum(Y * tf.log(Y_pred))\n",
    "    #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(Y_pred,Y))\n",
    "    cost =tf.sqrt(tf.reduce_mean(tf.square(tf.sub(Y_pred, Y))))\n",
    "    correct_prediction = tf.equal(tf.argmax(Y_pred, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    learning_rate = 0.01\n",
    "    batch_size=500\n",
    "    prediction=tf.argmax(Y_pred,1)   \n",
    "    prediction =tf.shape(prediction)\n",
    "  #  print prediction.eval(feed_dict={x: mnist.test.images})\n",
    "    datapoint_size=2000;\n",
    "#    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)    \n",
    "    print(cost)\n",
    "    prev_training_accuracy = 0.0\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, 'my-model')\n",
    "    for i in range(10):\n",
    "        for r in range(100):\n",
    "            if datapoint_size == batch_size:\n",
    "                batch_start_idx = 0\n",
    "            else:\n",
    "                batch_start_idx = (i * batch_size) % (datapoint_size - batch_size)\n",
    "                batch_end_idx = batch_start_idx + batch_size\n",
    "                xb=np.array(x_train[batch_start_idx:batch_end_idx])\n",
    "                yb=np.array(y_train[batch_start_idx:batch_end_idx])\n",
    "                #print(correct_prediction.eval(feed_dict={X: xb, Y:yb}))\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                        X: xb, Y:yb})\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "                #print(Y_pred.eval({X: x_train, Y:y_train}))\n",
    "                optimizer.run(feed_dict={X: x_train, Y:y_train})\n",
    "                prev_training_cost = train_accuracy\n",
    "    print(\"validation accuracy %g\"%accuracy.eval(feed_dict={\n",
    "                X: x_valid, Y:y_valid}))\n",
    "    print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "                X: x_test, Y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sqrt:0\", shape=(), dtype=float32)\n",
      "[[ 8.07319164]]\n",
      "[[ 7.69170141]]\n",
      "[[ 7.5717473]]\n",
      "[[ 7.06379986]]\n",
      "[[ 7.5310483]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZIk1TIkSJKkWoYESZJUy5AgSZJq/X/MjHAzo9DWvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f701c0e1b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #cost = -tf.reduce_sum(Y * tf.log(Y_pred))\n",
    "    #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(Y_pred,Y))\n",
    "    cost =tf.sqrt(tf.reduce_mean(tf.square(tf.sub(Y_pred, Y))))\n",
    "    correct_prediction = tf.equal(Y_pred,Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    learning_rate = 0.001\n",
    "    batch_size=500\n",
    "    prediction=tf.argmax(Y_pred,1)   \n",
    "    prediction =tf.shape(prediction)\n",
    "  #  print prediction.eval(feed_dict={x: mnist.test.images})\n",
    "    datapoint_size=2000;\n",
    "#    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)    \n",
    "    print(cost)\n",
    "    prev_training_accuracy = 0.0\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, 'my-model')\n",
    "    errors = []\n",
    "    for i in range(500):\n",
    "        for r in range(100):\n",
    "            if datapoint_size == batch_size:\n",
    "                batch_start_idx = 0\n",
    "            else:\n",
    "                batch_start_idx = (i * batch_size) % (datapoint_size - batch_size)\n",
    "                batch_end_idx = batch_start_idx + batch_size\n",
    "                xb=np.array(x_train[batch_start_idx:batch_end_idx])\n",
    "                yb=np.array(y_train[batch_start_idx:batch_end_idx])\n",
    "                #print(correct_prediction.eval(feed_dict={X: xb, Y:yb}))\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                        X: xb, Y:yb})\n",
    "               # print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "                errors.append(cost.eval({X: x_train, Y:y_train}))\n",
    "                optimizer.run(feed_dict={X: x_train, Y:y_train})\n",
    "             #   if(correct_prediction.eval(feed_dict={X: xb, Y:yb})==True):\n",
    "             #       print(Y);\n",
    "                prev_training_cost = train_accuracy\n",
    "#    print(\"validation accuracy %g\"%accuracy.eval(feed_dict={\n",
    "  #              X: x_valid, Y:y_valid}))\n",
    " #   print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "   #             X: x_test, Y:y_test}))\n",
    "    print(Y_pred.eval(feed_dict={X:np.reshape(x_train[3],[1,3]),Y:np.reshape(y_train[3],[1,1])}))\n",
    "    print(Y_pred.eval(feed_dict={X:np.reshape(x_train[1],[1,3]),Y:np.reshape(y_train[1],[1,1])}))    \n",
    "    print(Y_pred.eval(feed_dict={X:np.reshape(x_train[6],[1,3]),Y:np.reshape(y_train[6],[1,1])}))    \n",
    "    print(Y_pred.eval(feed_dict={X:np.reshape(x_train[7],[1,3]),Y:np.reshape(y_train[7],[1,1])}))\n",
    "    print(Y_pred.eval(feed_dict={X:np.reshape(x_train[9],[1,3]),Y:np.reshape(y_train[9],[1,1])}))\n",
    "    \n",
    "    plt.plot([np.mean(errors[i-20:i]) for i in range(len(errors))])\n",
    "    plt.show()\n",
    "    plt.savefig(\"errors.png\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
