{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "#import dataframe as df\n",
    "\n",
    "\n",
    "movie = pd.read_csv('movie_data_for_assignment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                      movie_title             color  \\\n",
       "0                                 Stuart Little              Color   \n",
       "1                                         Crash              Color   \n",
       "2              Ghost Rider: Spirit of Vengeance              Color   \n",
       "3                                    Gettysburg              Color   \n",
       "4                            Planet of the Apes              Color   \n",
       "5                         Daredevil                            NaN   \n",
       "6                      The Pursuit of Happyness              Color   \n",
       "7                           Kiss Kiss Bang Bang              Color   \n",
       "8                              Music and Lyrics              Color   \n",
       "9                               Horrible Bosses              Color   \n",
       "10                                    Blindness              Color   \n",
       "11                                    Cleopatra              Color   \n",
       "12                                  Out of Time              Color   \n",
       "13                        My Best Friend's Girl              Color   \n",
       "14                             Ruby in Paradise              Color   \n",
       "15                                    The Quiet              Color   \n",
       "16              The Powerpuff Girls                          Color   \n",
       "17                               Black November              Color   \n",
       "18             Dodgeball: A True Underdog Story    Black and White   \n",
       "19                          Grand Theft Parsons              Color   \n",
       "20                                    Losin' It              Color   \n",
       "21                                   Royal Kill              Color   \n",
       "22                                     Steamboy              Color   \n",
       "23                        The French Connection              Color   \n",
       "24               The Return of the Pink Panther              Color   \n",
       "25                         Flags of Our Fathers              Color   \n",
       "26                    El crimen del padre Amaro              Color   \n",
       "27                     The Secret in Their Eyes              Color   \n",
       "28                               Reservoir Dogs              Color   \n",
       "29                                  Chicken Run              Color   \n",
       "...                                          ...               ...   \n",
       "4270                                 Life of Pi              Color   \n",
       "4271                                    Frailty              Color   \n",
       "4272                        From Dusk Till Dawn              Color   \n",
       "4273                                Bright Star              Color   \n",
       "4274                          Final Destination              Color   \n",
       "4275                                 Changeling              Color   \n",
       "4276                         The Banger Sisters              Color   \n",
       "4277                                 Battleship              Color   \n",
       "4278                            Poltergeist III              Color   \n",
       "4279                         A Walk to Remember              Color   \n",
       "4280                                       Bang              Color   \n",
       "4281                    My Beautiful Laundrette              Color   \n",
       "4282                     300: Rise of an Empire              Color   \n",
       "4283                                    Bronson              Color   \n",
       "4284                          Hollywood Shuffle              Color   \n",
       "4285                        The Blood of Heroes              Color   \n",
       "4286                      House of 1000 Corpses    Black and White   \n",
       "4287                   Alexander's Ragtime Band    Black and White   \n",
       "4288                              Enter Nowhere              Color   \n",
       "4289                   Clear and Present Danger              Color   \n",
       "4290                           Agent Cody Banks              Color   \n",
       "4291                                Buffalo '66              Color   \n",
       "4292                              Anna Karenina              Color   \n",
       "4293                               Pandaemonium              Color   \n",
       "4294                         Out of the Furnace              Color   \n",
       "4295  The Twilight Saga: Breaking Dawn - Part 2              Color   \n",
       "4296             Diary of a Wimpy Kid: Dog Days              Color   \n",
       "4297             The Doombolt Chase                          Color   \n",
       "4298                                  Treachery              Color   \n",
       "4299                                    Get Low              Color   \n",
       "\n",
       "                director_name  num_critic_for_reviews  duration  \\\n",
       "0                 Rob Minkoff                   101.0      84.0   \n",
       "1                 Paul Haggis                   287.0     115.0   \n",
       "2              Mark Neveldine                   287.0      96.0   \n",
       "3                 Ron Maxwell                    22.0     271.0   \n",
       "4                  Tim Burton                   230.0     119.0   \n",
       "5                         NaN                    95.0      54.0   \n",
       "6            Gabriele Muccino                   201.0     117.0   \n",
       "7                 Shane Black                   223.0     103.0   \n",
       "8               Marc Lawrence                   175.0      95.0   \n",
       "9                 Seth Gordon                   340.0     106.0   \n",
       "10         Fernando Meirelles                   229.0     121.0   \n",
       "11       Joseph L. Mankiewicz                    72.0     251.0   \n",
       "12              Carl Franklin                   124.0     114.0   \n",
       "13              Howard Deutch                    80.0     112.0   \n",
       "14               Victor Nunez                    11.0     114.0   \n",
       "15               Jamie Babbit                    65.0      91.0   \n",
       "16                        NaN                     7.0      30.0   \n",
       "17                 Jeta Amata                     6.0      95.0   \n",
       "18    Rawson Marshall Thurber                   191.0      92.0   \n",
       "19              David Caffrey                    51.0      88.0   \n",
       "20              Curtis Hanson                     7.0     100.0   \n",
       "21                Babar Ahmed                     8.0      90.0   \n",
       "22            Katsuhiro Ôtomo                   105.0     103.0   \n",
       "23           William Friedkin                   138.0     104.0   \n",
       "24              Blake Edwards                    33.0     113.0   \n",
       "25             Clint Eastwood                   279.0     135.0   \n",
       "26             Carlos Carrera                    85.0     118.0   \n",
       "27       Juan José Campanella                   262.0     129.0   \n",
       "28          Quentin Tarantino                   173.0      99.0   \n",
       "29                 Peter Lord                   187.0      84.0   \n",
       "...                       ...                     ...       ...   \n",
       "4270                  Ang Lee                   552.0     127.0   \n",
       "4271              Bill Paxton                   161.0     100.0   \n",
       "4272         Robert Rodriguez                   165.0     108.0   \n",
       "4273             Jane Campion                   222.0     119.0   \n",
       "4274               James Wong                   211.0      98.0   \n",
       "4275           Clint Eastwood                   264.0     141.0   \n",
       "4276               Bob Dolman                    92.0      98.0   \n",
       "4277               Peter Berg                   377.0     131.0   \n",
       "4278             Gary Sherman                    66.0      98.0   \n",
       "4279            Adam Shankman                    80.0     101.0   \n",
       "4280          Ash Baron-Cohen                    10.0      98.0   \n",
       "4281           Stephen Frears                    46.0      97.0   \n",
       "4282               Noam Murro                   366.0     102.0   \n",
       "4283     Nicolas Winding Refn                   225.0      92.0   \n",
       "4284          Robert Townsend                    21.0      81.0   \n",
       "4285       David Webb Peoples                    21.0      90.0   \n",
       "4286               Rob Zombie                   202.0     105.0   \n",
       "4287               Henry King                    10.0     106.0   \n",
       "4288              Jack Heller                    26.0      90.0   \n",
       "4289            Phillip Noyce                    42.0     141.0   \n",
       "4290             Harald Zwart                    79.0     102.0   \n",
       "4291            Vincent Gallo                   106.0     110.0   \n",
       "4292               Joe Wright                   324.0     129.0   \n",
       "4293            Julien Temple                    22.0     124.0   \n",
       "4294             Scott Cooper                   288.0     116.0   \n",
       "4295              Bill Condon                   322.0     115.0   \n",
       "4296             David Bowers                    88.0      94.0   \n",
       "4297                      NaN                     NaN      30.0   \n",
       "4298            Travis Romero                     5.0      67.0   \n",
       "4299          Aaron Schneider                   160.0     100.0   \n",
       "\n",
       "      director_facebook_likes  actor_3_facebook_likes  \\\n",
       "0                        50.0                   692.0   \n",
       "1                       549.0                   911.0   \n",
       "2                        83.0                  1000.0   \n",
       "3                        33.0                   251.0   \n",
       "4                     13000.0                   567.0   \n",
       "5                         NaN                     0.0   \n",
       "6                       125.0                   168.0   \n",
       "7                      1000.0                   611.0   \n",
       "8                        30.0                   664.0   \n",
       "9                        91.0                    64.0   \n",
       "10                      353.0                    18.0   \n",
       "11                      311.0                   595.0   \n",
       "12                       73.0                   323.0   \n",
       "13                       41.0                   490.0   \n",
       "14                        9.0                    86.0   \n",
       "15                       91.0                   659.0   \n",
       "16                        NaN                   265.0   \n",
       "17                       20.0                    36.0   \n",
       "18                       43.0                   936.0   \n",
       "19                        4.0                    95.0   \n",
       "20                      161.0                   172.0   \n",
       "21                        0.0                    32.0   \n",
       "22                       78.0                   101.0   \n",
       "23                      607.0                   109.0   \n",
       "24                      688.0                    80.0   \n",
       "25                    16000.0                   310.0   \n",
       "26                       12.0                    67.0   \n",
       "27                      195.0                    50.0   \n",
       "28                    16000.0                   455.0   \n",
       "29                       91.0                   206.0   \n",
       "...                       ...                     ...   \n",
       "4270                      0.0                   341.0   \n",
       "4271                      0.0                   303.0   \n",
       "4272                      0.0                   844.0   \n",
       "4273                    319.0                   179.0   \n",
       "4274                     70.0                   591.0   \n",
       "4275                  16000.0                   539.0   \n",
       "4276                      7.0                    73.0   \n",
       "4277                    532.0                   627.0   \n",
       "4278                     17.0                   770.0   \n",
       "4279                    163.0                   488.0   \n",
       "4280                      3.0                   152.0   \n",
       "4281                    350.0                    33.0   \n",
       "4282                    263.0                  1000.0   \n",
       "4283                      0.0                   146.0   \n",
       "4284                    467.0                   287.0   \n",
       "4285                     73.0                    26.0   \n",
       "4286                      0.0                   303.0   \n",
       "4287                     85.0                   300.0   \n",
       "4288                      0.0                   120.0   \n",
       "4289                    176.0                   672.0   \n",
       "4290                     91.0                   434.0   \n",
       "4291                    787.0                   642.0   \n",
       "4292                    456.0                     0.0   \n",
       "4293                     67.0                   303.0   \n",
       "4294                    108.0                   120.0   \n",
       "4295                    386.0                 12000.0   \n",
       "4296                     42.0                   517.0   \n",
       "4297                      NaN                     9.0   \n",
       "4298                      4.0                  2000.0   \n",
       "4299                     11.0                   970.0   \n",
       "\n",
       "                 actor_2_name  actor_1_facebook_likes        gross    ...      \\\n",
       "0                 Nathan Lane                   979.0  140015224.0    ...       \n",
       "1              Loretta Devine                  3000.0   54557348.0    ...       \n",
       "2             Spencer Wilding                 12000.0   51774002.0    ...       \n",
       "3     William Morgan Sheppard                   854.0   10769960.0    ...       \n",
       "4              Estella Warren                  1000.0  180011740.0    ...       \n",
       "5               Royce Johnson                   577.0          NaN    ...       \n",
       "6                 Kurt Fuller                 10000.0  162586036.0    ...       \n",
       "7              Corbin Bernsen                 21000.0    4235837.0    ...       \n",
       "8                Scott Porter                   799.0   50562555.0    ...       \n",
       "9              Lindsay Sloane                 18000.0  117528646.0    ...       \n",
       "10                 Joe Pingue                    45.0    3073392.0    ...       \n",
       "11             Richard Burton                   940.0   57750000.0    ...       \n",
       "12               Sanaa Lathan                 18000.0   40905277.0    ...       \n",
       "13               Taran Killam                  1000.0   19151864.0    ...       \n",
       "14                 Todd Field                   159.0    1001437.0    ...       \n",
       "15            David Gallagher                   982.0     381186.0    ...       \n",
       "16              Jennifer Hale                   971.0          NaN    ...       \n",
       "17              Nathin Butler                   262.0          NaN    ...       \n",
       "18               Stephen Root                   989.0  114324072.0    ...       \n",
       "19                Scott Adsit                   690.0          NaN    ...       \n",
       "20               Shelley Long                 10000.0          NaN    ...       \n",
       "21           Alexander Wraith                   502.0          NaN    ...       \n",
       "22         Robin Atkin Downes                   488.0     410388.0    ...       \n",
       "23               Fernando Rey                   813.0          NaN    ...       \n",
       "24                Herbert Lom                   462.0          NaN    ...       \n",
       "25                Chris Bauer                 23000.0   33574332.0    ...       \n",
       "26       Ana Claudia Talancón                   201.0    5709616.0    ...       \n",
       "27           Soledad Villamil                   827.0   20167424.0    ...       \n",
       "28              Steve Buscemi                 16000.0    2812029.0    ...       \n",
       "29         Miranda Richardson                   579.0  106793915.0    ...       \n",
       "...                       ...                     ...          ...    ...       \n",
       "4270               Rafe Spall                   774.0  124976634.0    ...       \n",
       "4271            Powers Boothe                 11000.0   13103828.0    ...       \n",
       "4272              Salma Hayek                 16000.0   25753840.0    ...       \n",
       "4273           Paul Schneider                  2000.0    4440055.0    ...       \n",
       "4274             Brendan Fehr                  1000.0   53302314.0    ...       \n",
       "4275            Michael Kelly                 11000.0   35707327.0    ...       \n",
       "4276       Eva Amurri Martino                   931.0   30306281.0    ...       \n",
       "4277      Alexander Skarsgård                 14000.0   65173160.0    ...       \n",
       "4278         Heather O'Rourke                  1000.0   14114488.0    ...       \n",
       "4279             Peter Coyote                   683.0   41227069.0    ...       \n",
       "4280        Stanley B. Herman                   789.0          NaN    ...       \n",
       "4281              Roshan Seth                   114.0          NaN    ...       \n",
       "4282       Sullivan Stapleton                  6000.0  106369117.0    ...       \n",
       "4283              James Lance                 27000.0     104792.0    ...       \n",
       "4284      Keenen Ivory Wayans                   467.0    5228617.0    ...       \n",
       "4285                Joan Chen                   848.0     882290.0    ...       \n",
       "4286          Matthew McGrory                  1000.0   12583510.0    ...       \n",
       "4287               Don Ameche                   480.0          NaN    ...       \n",
       "4288      Katherine Waterston                   322.0          NaN    ...       \n",
       "4289               Dean Jones                 11000.0  122012710.0    ...       \n",
       "4290            Frankie Muniz                  1000.0   47285499.0    ...       \n",
       "4291            Vincent Gallo                  1000.0    2365931.0    ...       \n",
       "4292      Guro Nagelhus Schia                  2000.0   12802907.0    ...       \n",
       "4293          Dexter Fletcher                   631.0          NaN    ...       \n",
       "4294              Sam Shepard                 23000.0   11326836.0    ...       \n",
       "4295          Kristen Stewart                 21000.0  292298923.0    ...       \n",
       "4296           Rachael Harris                   975.0   49002815.0    ...       \n",
       "4297         George Coulouris                   310.0          NaN    ...       \n",
       "4298            Lorraine Ziff                260000.0          NaN    ...       \n",
       "4299            Robert Duvall                 13000.0    9176553.0    ...       \n",
       "\n",
       "     num_user_for_reviews  language    country  content_rating        budget  \\\n",
       "0                   179.0   English    Germany              PG  1.330000e+08   \n",
       "1                  1624.0   English    Germany               R  6.500000e+06   \n",
       "2                   331.0   English        USA           PG-13  5.700000e+07   \n",
       "3                   256.0   English        USA              PG  2.500000e+07   \n",
       "4                  1368.0   English        USA           PG-13  1.000000e+08   \n",
       "5                   394.0   English        USA           TV-MA           NaN   \n",
       "6                   611.0   English        USA           PG-13  5.500000e+07   \n",
       "7                   336.0   English        USA               R  1.500000e+07   \n",
       "8                   291.0   English        USA           PG-13           NaN   \n",
       "9                   387.0   English        USA               R  3.500000e+07   \n",
       "10                  300.0   English     Canada               R  2.500000e+07   \n",
       "11                  192.0   English         UK        Approved  3.111500e+07   \n",
       "12                  157.0   English        USA               R  5.000000e+07   \n",
       "13                  100.0   English        USA               R  4.000000e+07   \n",
       "14                   28.0   English        USA               R  8.000000e+05   \n",
       "15                   90.0   English        USA               R  9.000000e+05   \n",
       "16                   60.0   English        USA           TV-Y7           NaN   \n",
       "17                    4.0   English    Nigeria             NaN  7.500000e+06   \n",
       "18                  392.0   English        USA           PG-13  2.000000e+07   \n",
       "19                   38.0   English        USA           PG-13           NaN   \n",
       "20                   16.0   English     Canada               R  7.000000e+06   \n",
       "21                   18.0   English        USA           PG-13  3.500000e+05   \n",
       "22                   79.0  Japanese      Japan           PG-13  2.127520e+09   \n",
       "23                  280.0   English        USA               R  1.800000e+06   \n",
       "24                   73.0   English         UK               G  5.000000e+06   \n",
       "25                  415.0   English        USA               R  9.000000e+07   \n",
       "26                  110.0   Spanish     Mexico               R  1.800000e+06   \n",
       "27                  231.0   Spanish  Argentina               R  2.000000e+06   \n",
       "28                  931.0   English        USA               R  1.200000e+06   \n",
       "29                  358.0   English         UK               G  4.500000e+07   \n",
       "...                   ...       ...        ...             ...           ...   \n",
       "4270                755.0   English        USA              PG  1.200000e+08   \n",
       "4271                463.0   English        USA               R  1.100000e+07   \n",
       "4272                592.0   English        USA               R  1.900000e+07   \n",
       "4273                110.0   English         UK              PG  8.500000e+06   \n",
       "4274                674.0   English        USA               R  2.300000e+07   \n",
       "4275                387.0   English        USA               R  5.500000e+07   \n",
       "4276                147.0   English        USA               R  1.000000e+07   \n",
       "4277                751.0   English        USA           PG-13  2.090000e+08   \n",
       "4278                114.0   English        USA           PG-13  1.050000e+07   \n",
       "4279                962.0   English        USA              PG  1.100000e+07   \n",
       "4280                 14.0   English        USA             NaN           NaN   \n",
       "4281                 64.0   English         UK               R  6.500000e+05   \n",
       "4282                523.0   English        USA               R  1.100000e+08   \n",
       "4283                145.0   English         UK               R           NaN   \n",
       "4284                 32.0   English        USA               R  1.000000e+05   \n",
       "4285                 43.0   English  Australia               R  1.000000e+07   \n",
       "4286                922.0   English        USA               R  7.000000e+06   \n",
       "4287                 29.0   English        USA        Approved  2.000000e+06   \n",
       "4288                 45.0   English        USA               R  5.000000e+05   \n",
       "4289                133.0   English        USA           PG-13  6.200000e+07   \n",
       "4290                104.0   English        USA              PG  2.600000e+07   \n",
       "4291                318.0   English        USA               R  1.500000e+06   \n",
       "4292                226.0   English         UK               R           NaN   \n",
       "4293                 32.0   English         UK           PG-13  1.500000e+07   \n",
       "4294                231.0   English        USA               R  2.200000e+07   \n",
       "4295                329.0   English        USA           PG-13  1.200000e+08   \n",
       "4296                 35.0   English        USA              PG  2.200000e+07   \n",
       "4297                  NaN   English         UK             NaN           NaN   \n",
       "4298                  5.0   English        USA             NaN  6.250000e+05   \n",
       "4299                 97.0   English        USA           PG-13  7.500000e+06   \n",
       "\n",
       "      title_year actor_2_facebook_likes aspect_ratio  movie_facebook_likes  \\\n",
       "0         1999.0                  886.0         1.85                     0   \n",
       "1         2004.0                  912.0         2.35                 18000   \n",
       "2         2011.0                 1000.0         2.35                 18000   \n",
       "3         1993.0                  702.0         1.85                     0   \n",
       "4         2001.0                  658.0         2.35                     0   \n",
       "5            NaN                    4.0        16.00                 55000   \n",
       "6         2006.0                  617.0         2.35                 32000   \n",
       "7         2005.0                 1000.0         2.35                     0   \n",
       "8         2007.0                  690.0         1.85                     0   \n",
       "9         2011.0                  464.0         2.35                 31000   \n",
       "10        2008.0                   30.0         1.85                     0   \n",
       "11        1963.0                  726.0         2.20                     0   \n",
       "12        2003.0                  886.0         2.35                  1000   \n",
       "13        2008.0                  500.0         1.85                     0   \n",
       "14        1993.0                  143.0         1.85                    81   \n",
       "15        2005.0                  796.0         2.35                   698   \n",
       "16           NaN                  918.0         4.00                   581   \n",
       "17        2012.0                   65.0          NaN                   389   \n",
       "18        2004.0                  939.0         2.35                     0   \n",
       "19        2003.0                  316.0         1.85                   480   \n",
       "20        1983.0                  422.0          NaN                   168   \n",
       "21        2009.0                  119.0          NaN                    53   \n",
       "22        2004.0                  336.0         1.85                   973   \n",
       "23        1971.0                  165.0         1.85                     0   \n",
       "24        1975.0                  278.0         2.35                   620   \n",
       "25        2006.0                  638.0         2.35                     0   \n",
       "26        2002.0                  163.0         1.85                   544   \n",
       "27        2009.0                   88.0         2.35                 33000   \n",
       "28        1992.0                12000.0         2.35                 19000   \n",
       "29        2000.0                  530.0         1.85                     0   \n",
       "...          ...                    ...          ...                   ...   \n",
       "4270      2012.0                  358.0         1.85                122000   \n",
       "4271      2001.0                  472.0         1.85                  5000   \n",
       "4272      1996.0                 4000.0         1.85                 12000   \n",
       "4273      2009.0                  552.0         1.85                     0   \n",
       "4274      2000.0                  847.0         1.85                     0   \n",
       "4275      2008.0                  963.0         2.35                 14000   \n",
       "4276      2002.0                  797.0         2.35                   744   \n",
       "4277      2012.0                10000.0         2.35                 44000   \n",
       "4278      1988.0                  887.0         1.85                   616   \n",
       "4279      2002.0                  548.0         2.35                 19000   \n",
       "4280      1995.0                  194.0          NaN                    20   \n",
       "4281      1985.0                   61.0         1.66                     0   \n",
       "4282      2014.0                 1000.0         2.35                 71000   \n",
       "4283      2008.0                  161.0         1.85                 22000   \n",
       "4284      1987.0                  322.0         1.85                   471   \n",
       "4285      1989.0                  643.0         1.33                   999   \n",
       "4286      2003.0                  340.0         1.85                     0   \n",
       "4287      1938.0                  392.0         1.37                    60   \n",
       "4288      2011.0                  178.0          NaN                   701   \n",
       "4289      1994.0                  913.0         2.35                     0   \n",
       "4290      2003.0                  934.0         2.35                   542   \n",
       "4291      1998.0                  787.0         1.85                     0   \n",
       "4292      2012.0                   35.0         2.35                 36000   \n",
       "4293      2000.0                  452.0          NaN                   277   \n",
       "4294      2013.0                  820.0         2.35                 17000   \n",
       "4295      2012.0                17000.0         2.35                 65000   \n",
       "4296      2012.0                  569.0         2.35                     0   \n",
       "4297         NaN                   11.0          NaN                     0   \n",
       "4298      2013.0                21000.0          NaN                     0   \n",
       "4299      2009.0                 3000.0         2.35                     0   \n",
       "\n",
       "     imdb_score  \n",
       "0           5.9  \n",
       "1           7.9  \n",
       "2           4.3  \n",
       "3           7.7  \n",
       "4           5.7  \n",
       "5           8.8  \n",
       "6           8.0  \n",
       "7           7.6  \n",
       "8           6.5  \n",
       "9           6.9  \n",
       "10          6.6  \n",
       "11          7.0  \n",
       "12          6.5  \n",
       "13          5.9  \n",
       "14          7.2  \n",
       "15          6.4  \n",
       "16          7.2  \n",
       "17          5.6  \n",
       "18          6.7  \n",
       "19          6.1  \n",
       "20          4.8  \n",
       "21          3.2  \n",
       "22          6.9  \n",
       "23          7.8  \n",
       "24          7.1  \n",
       "25          7.1  \n",
       "26          6.8  \n",
       "27          8.2  \n",
       "28          8.4  \n",
       "29          7.0  \n",
       "...         ...  \n",
       "4270        8.0  \n",
       "4271        7.3  \n",
       "4272        7.3  \n",
       "4273        7.0  \n",
       "4274        6.7  \n",
       "4275        7.8  \n",
       "4276        5.6  \n",
       "4277        5.9  \n",
       "4278        4.5  \n",
       "4279        7.4  \n",
       "4280        6.4  \n",
       "4281        6.9  \n",
       "4282        6.2  \n",
       "4283        7.1  \n",
       "4284        7.0  \n",
       "4285        6.5  \n",
       "4286        6.0  \n",
       "4287        7.0  \n",
       "4288        6.6  \n",
       "4289        6.9  \n",
       "4290        5.0  \n",
       "4291        7.5  \n",
       "4292        6.6  \n",
       "4293        6.6  \n",
       "4294        6.8  \n",
       "4295        5.5  \n",
       "4296        6.4  \n",
       "4297        7.2  \n",
       "4298        3.9  \n",
       "4299        7.1  \n",
       "\n",
       "[4300 rows x 28 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_list = [] # empty list to contain columns with strings (words)\n",
    "for colname, colvalue in movie.iteritems():\n",
    "    if type(colvalue[1]) == str:\n",
    "         str_list.append(colname)\n",
    "# Get to the numeric columns by inversion            \n",
    "num_list = movie.columns.difference(str_list)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4300, 16)\n",
      "(4300, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "movie_num = movie[num_list]\n",
    "#del movie # Get rid of movie df as we won't need it now\n",
    "movie_num.head()\n",
    "numpyMatrix = movie_num.as_matrix()\n",
    "print(numpyMatrix.shape)\n",
    "y_std = numpyMatrix[:, [10]]\n",
    "nump1 = np.delete(numpyMatrix, [10], axis=1)\n",
    "movie_num.shape;\n",
    "print(nump1.shape)\n",
    "movie_num1=movie_num.drop('imdb_score', axis=1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_num = movie_num.fillna(value=0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_num = movie_num.fillna(value=0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.9]\n",
      " [ 7.9]\n",
      " [ 4.3]\n",
      " ..., \n",
      " [ 7.2]\n",
      " [ 3.9]\n",
      " [ 7.1]]\n"
     ]
    }
   ],
   "source": [
    "X = movie_num.values\n",
    "# Data Normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "#y_std=X_std[:, [15]]\n",
    "print(y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort from high to low\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "# Calculation of Explained Variance from the eigenvalues\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "x_9d = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4300, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_9d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys = y_std\n",
    "xs = x_9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = xs[range(3300),:]\n",
    "y_train = ys[range(3300),:]\n",
    "#x_valid = xs[range(2300,3300),:]\n",
    "#y_valid = ys[range(2300,3300),:]\n",
    "x_test = xs[range(3300,4300),:]\n",
    "y_test = ys[range(3300,4300),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "n_observations = 2300\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x_train = xs[range(2300),:]\n",
    "y_train = ys[range(2300),:]\n",
    "x_valid = xs[range(2300,3300),:]\n",
    "y_valid = ys[range(2300,3300),:]\n",
    "x_test = xs[range(3300,3400),:]\n",
    "y_test = ys[range(3300,3400),:]\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %% Let's create some toy data\n",
    "plt.ion()\n",
    "n_observations = 2300\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x_train = xs[range(3300),:]\n",
    "y_train = ys[range(3300),:]\n",
    "#x_valid = xs[range(2300,3300),:]\n",
    "#y_valid = ys[range(2300,3300),:]\n",
    "x_test = xs[range(3300,4300),:]\n",
    "y_test = ys[range(3300,4300),:]\n",
    "print(y_test.shape)\n",
    "#xs = np.linspace(-3, 3, n_observations)\n",
    "#ys = np.sin(xs) + np.random.uniform(-0.5, 0.5, n_observations)\n",
    "#ax.scatter(xs, ys)\n",
    "#fig.show()\n",
    "#plt.draw()\n",
    "\n",
    "# %% tf.placeholders for the input and output of the network. Placeholders are\n",
    "# variables which we need to fill in when we are ready to compute the graph.\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# %% We will try to optimize min_(W,b) ||(X*w + b) - y||^2\n",
    "# The `Variable()` constructor requires an initial value for the variable,\n",
    "# which can be a `Tensor` of any type and shape. The initial value defines the\n",
    "# type and shape of the variable. After construction, the type and shape of\n",
    "# the variable are fixed. The value can be changed using one of the assign\n",
    "# methods.\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "Y_pred = tf.add(tf.mul(X, W), b)\n",
    "\n",
    "\n",
    "# %% Loss function will measure the distance between our observations\n",
    "# and predictions and average over them.\n",
    "cost = tf.reduce_sum(tf.pow(Y_pred - Y, 2)) / (n_observations - 1)\n",
    "\n",
    "# %% if we wanted to add regularization, we could add other terms to the cost,\n",
    "# e.g. ridge regression has a parameter controlling the amount of shrinkage\n",
    "# over the norm of activations. the larger the shrinkage, the more robust\n",
    "# to collinearity.\n",
    "# cost = tf.add(cost, tf.mul(1e-6, tf.global_norm([W])))\n",
    "\n",
    "# %% Use gradient descent to optimize W,b\n",
    "# Performs a single step in the negative gradient\n",
    "correct_prediction = tf.equal(Y_pred,Y);\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# %% We create a session to use the graph\n",
    "n_epochs = 100\n",
    "with tf.Session() as sess:\n",
    "    # Here we tell tensorflow that we want to initialize all\n",
    "    # the variables in the graph so we can use them\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Fit all training data\n",
    "    prev_training_cost = 0.0\n",
    "    for epoch_i in range(n_epochs):\n",
    "        for (x, y) in zip(x_train, y_train):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        training_cost = sess.run(\n",
    "            cost, feed_dict={X: x_train, Y: y_train})\n",
    "        print(training_cost)\n",
    "        if np.abs(prev_training_cost - training_cost) < 0.000001:\n",
    "         break\n",
    "        prev_training_cost = training_cost\n",
    "        \n",
    "  #  print(sess.run(accuracy,\n",
    "   #     feed_dict={\n",
    "    #    X: x_valid,\n",
    "     #   Y: y_valid\n",
    "      #  }))\n",
    "\n",
    "# %% Print final test accuracy:\n",
    "  #  print(sess.run(accuracy,\n",
    "   #     feed_dict={\n",
    "    #    X: x_test,\n",
    "     #   Y: y_test\n",
    "      # }))\n",
    "        testing_cost = sess.run(\n",
    "        tf.reduce_sum(tf.pow(Y_pred - Y, 2)) / (2 * x_test.shape[0]),\n",
    "        feed_dict={X:x_test, Y:y_test})  # same function as cost above\n",
    "    print(\"Testing cost=\", testing_cost)\n",
    "    print(\"Absolute mean square loss difference:\", abs(\n",
    "        training_cost - testing_cost)) \n",
    "        \n",
    "    plt.plot(x_test, y_test, 'bo', label='Testing data')\n",
    "    plt.plot(x_train, sess.run(W) * x_train + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "       # if epoch_i % 20 == 0:\n",
    "        #    ax.plot(xs, Y_pred.eval(\n",
    "        #        feed_dict={X: xs}, session=sess),\n",
    "         #           'k', alpha=epoch_i / n_epochs)\n",
    "         #   fig.show()\n",
    "         #   plt.draw()\n",
    "\n",
    "        # Allow the training to quit if we've reached a minimum\n",
    "       # if np.abs(prev_training_cost - training_cost) < 0.000001:\n",
    "       #     break\n",
    "       # prev_training_cost = training_cost\n",
    "#fig.show()\n",
    "#plt.waitforbuttonpress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed):\n",
    "    np.random.seed(seed)\n",
    "    slices = []\n",
    "    centroids = []\n",
    "    # Create samples for each cluster\n",
    "    for i in range(n_clusters):\n",
    "        samples = tf.random_normal((n_samples_per_cluster, n_features),\n",
    "                               mean=0.0, stddev=5.0, dtype=tf.float32, seed=seed, name=\"cluster_{}\".format(i))\n",
    "        current_centroid = (np.random.random((1, n_features)) * embiggen_factor) - (embiggen_factor/2)\n",
    "        centroids.append(current_centroid)\n",
    "        samples += current_centroid\n",
    "        slices.append(samples)\n",
    "    # Create a big \"samples\" dataset\n",
    "    samples = tf.concat(0, slices, name='samples')\n",
    "    centroids = tf.concat(0, centroids, name='centroids')\n",
    "   # xs = tf.cast(xs, dtype=\"float32\", name=None)\n",
    "    return centroids, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_features = 2\n",
    "n_clusters = 3\n",
    "n_samples_per_cluster = 500\n",
    "seed = 700\n",
    "embiggen_factor = 70\n",
    "np.random.seed(seed)\n",
    "\n",
    "centroids, samples = create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed)\n",
    "samples = tf.convert_to_tensor(samples)\n",
    "model = tf.initialize_all_variables()\n",
    "with tf.Session() as session:\n",
    "    sample_values = session.run(samples)\n",
    "    centroid_values = session.run(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_clusters(all_samples, centroids, n_samples_per_cluster):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Plot out the different clusters\n",
    "    # Choose a different colour for each cluster\n",
    "    colour = plt.cm.rainbow(np.linspace(0,1,len(centroids)))\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        # Grab just the samples fpr the given cluster and plot them out with a new colour\n",
    "        samples = all_samples[i*n_samples_per_cluster:(i+1)*n_samples_per_cluster]\n",
    "        plt.scatter(samples[:,0], samples[:,1], c=colour[i])\n",
    "        # Also plot centroid\n",
    "        plt.plot(centroid[0], centroid[1], markersize=35, marker=\"x\", color='k', mew=10)\n",
    "        plt.plot(centroid[0], centroid[1], markersize=30, marker=\"x\", color='m', mew=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_random_centroids(samples, n_clusters):\n",
    "    # Step 0: Initialisation: Select `n_clusters` number of random points\n",
    "    n_samples = tf.shape(samples)[0]\n",
    "    random_indices = tf.random_shuffle(tf.range(0, n_samples))\n",
    "    begin = [0,]\n",
    "    size = [n_clusters,]\n",
    "    size[0] = n_clusters\n",
    "    centroid_indices = tf.slice(random_indices, begin, size)\n",
    "    initial_centroids = tf.gather(samples, centroid_indices)\n",
    "    return initial_centroids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_clusters = 3\n",
    "n_samples_per_cluster = 500\n",
    "seed = 700\n",
    "embiggen_factor = 70\n",
    "\n",
    "centroids, samples = create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed)\n",
    "initial_centroids = choose_random_centroids(samples, n_clusters)\n",
    "xs = tf.convert_to_tensor(xs)\n",
    "model = tf.initialize_all_variables()\n",
    "with tf.Session() as session:\n",
    "    sample_values = session.run(xs)\n",
    "    updated_centroid_value = session.run(initial_centroids)\n",
    "\n",
    "#plot_clusters(sample_values, updated_centroid_value, n_samples_per_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_to_nearest(samples, centroids):\n",
    "    # Finds the nearest centroid for each sample\n",
    "\n",
    "    # START from http://esciencegroup.com/2016/01/05/an-encounter-with-googles-tensorflow/\n",
    "    expanded_vectors = tf.expand_dims(samples, 0)\n",
    "    expanded_centroids = tf.expand_dims(centroids, 1)\n",
    "    distances = tf.reduce_sum( tf.square(\n",
    "               tf.sub(expanded_vectors, expanded_centroids)), 2)\n",
    "    mins = tf.argmin(distances, 0)\n",
    "    # END from http://e}sciencegroup.com/2016/01/05/an-encounter-with-googles-tensorflow/\n",
    "    nearest_indices = mins\n",
    "    return nearest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_centroids(samples, nearest_indices, n_clusters):\n",
    "    # Updates the centroid to be the mean of all samples associated with it.\n",
    "    nearest_indices = tf.to_int32(nearest_indices)\n",
    "    partitions = tf.dynamic_partition(samples, nearest_indices, n_clusters)\n",
    "    new_centroids = tf.concat(0, [tf.expand_dims(tf.reduce_mean(partition, 0), 0) for partition in partitions])\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "n_features = 2\n",
    "n_clusters = 3\n",
    "n_samples_per_cluster = 500\n",
    "seed = 700\n",
    "embiggen_factor = 70\n",
    "\n",
    "xs = tf.cast(xs, dtype=\"float32\", name=None)\n",
    "#data_centroids, samples = create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed)\n",
    "initial_centroids = choose_random_centroids(xs, n_clusters)\n",
    "nearest_indices = assign_to_nearest(xs, initial_centroids)\n",
    "updated_centroids = update_centroids(xs, nearest_indices, n_clusters)\n",
    "for centroids_num in range(32):\n",
    "    model = tf.initialize_all_variables()\n",
    "    with tf.Session() as session:\n",
    "        sample_values = session.run(xs)\n",
    "        updated_centroid_value = session.run(updated_centroids)\n",
    "        print(updated_centroid_value)\n",
    "\n",
    "        plot_clusters(sample_values, updated_centroid_value, n_samples_per_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "print(updated_centroid_value.shape)\n",
    "print(type(nearest_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax.scatter(xs[:,[0]],xs[:,[1]],ys)\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "kmeans = KMeans(n_clusters=3)\n",
    "# Compute cluster centers and predict cluster indices\n",
    "X_clustered = kmeans.fit_predict(xs)\n",
    "print(X_clustered.shape)\n",
    "# Define our own color map\n",
    "LABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b'}\n",
    "label_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n",
    "\n",
    "# Plot the scatter digram\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.scatter(xs[:,0],xs[:,1], c= label_color, alpha=0.5) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,name=\"X\",shape=[None,2]);\n",
    "W = tf.Variable(tf.random_normal([2,5],dtype=tf.float32,stddev = 0.1,name =\"W\"))\n",
    "h = tf.matmul(X,W);\n",
    "b = tf.Variable(tf.constant([0,1],dtype=tf.float32,shape=[5],name = \"b\"));\n",
    "h = tf.nn.bias_add(h,b);\n",
    "h = tf.nn.relu(h);\n",
    "from libs import utils\n",
    "def linear(x, n_output, name=None, activation=None, reuse=None):\n",
    "    \"\"\"Fully connected layer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        Input tensor to connect\n",
    "    n_output : int\n",
    "        Number of output neurons\n",
    "    name : None, optional\n",
    "        Scope to apply\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    op : tf.Tensor\n",
    "        Output of fully connected layer.\n",
    "    \"\"\"\n",
    "    if len(x.get_shape()) != 2:\n",
    "        x = flatten(x, reuse=reuse)\n",
    "\n",
    "    n_input = x.get_shape().as_list()[1]\n",
    "\n",
    "    with tf.variable_scope(name or \"fc\", reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='W',\n",
    "            shape=[n_input, n_output],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        b = tf.get_variable(\n",
    "            name='b',\n",
    "            shape=[n_output],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        h = tf.nn.bias_add(\n",
    "            name='h',\n",
    "            value=tf.matmul(x, W),\n",
    "            bias=b)\n",
    "\n",
    "        if activation:\n",
    "            h = activation(h)\n",
    "\n",
    "        return h, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a placeholder of None x 2 dimensions and dtype tf.float32\n",
    "# This will be the input to the network which takes the row/col\n",
    "X = tf.placeholder(tf.float32,name=\"X\",shape=[None,2]);# Create the placeholder, Y, with 3 output dimensions instead of 2.\n",
    "# This will be the output of the network, the R, G, B values.\n",
    "Y = tf.placeholder(tf.float32,name=\"Y\",shape=[None,10]);\n",
    "labels = tf.placeholder(tf.float32,name=\"labels\",shape=[None,10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll create 6 hidden layers.  Let's create a variable\n",
    "# to say how many neurons we want for each of the layers\n",
    "# (try 20 to begin with, then explore other values)\n",
    "n_neurons = 20;\n",
    "\n",
    "# Create the first linear + nonlinear layer which will\n",
    "# take the 2 input neurons and fully connects it to 20 neurons.\n",
    "# Use the `utils.linear` function to do this just like before,\n",
    "# but also remember to give names for each layer, such as\n",
    "# \"1\", \"2\", ... \"5\", or \"layer1\", \"layer2\", ... \"layer6\".\n",
    "h1, W1 = linear(\n",
    "    x=X, n_output=20, name='linear1', activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "# Create another one:\n",
    "h2, W2 = linear(\n",
    "    x=h1, n_output=20, name='linear2', activation=tf.nn.relu)\n",
    "\n",
    "# and four more (or replace all of this with a loop if you can!):\n",
    "h3, W3 =  linear(\n",
    "    x=h2, n_output=20, name='linear3', activation=tf.nn.relu)\n",
    "h4, W4 =  linear(\n",
    "    x=h3, n_output=20, name='linear4', activation=tf.nn.relu)\n",
    "h5, W5 =  linear(\n",
    "    x=h4, n_output=20, name='linear5', activation=tf.nn.relu)\n",
    "h6, W6 =  linear(\n",
    "    x=h5, n_output=20, name='linear6', activation=tf.nn.relu)\n",
    "\n",
    "# Now, make one last layer to make sure your network has 3 outputs:\n",
    "Y_pred, W7 = linear(h6, 10, activation=None, name='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "print(ys[0])\n",
    "len = ys.shape[0];\n",
    "for r in range(len):\n",
    "    ys[r]= math.ceil(ys[r])\n",
    "    inputs, num_labels = ys[r], 10\n",
    "    indexedInputs = [[i, inputs[i]] for i in range(inputs.shape[0])]\n",
    "    indexedInputs =tf.cast(indexedInputs, tf.int32)\n",
    "    with tf.Session() as session:\n",
    "        print(indexedInputs.eval())\n",
    "        ys[r]=tf.sparse_to_dense(indexedInputs[0],indexedInputs[1] ,[1,num_labels], 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "for r in range(4300):\n",
    "    ys[r]= math.ceil(ys[r])\n",
    "print(ys.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys = np.asarray(ys)\n",
    "num_labels = 10\n",
    "sparse_labels = tf.reshape(ys, [-1, 1])\n",
    "derived_size = tf.shape(ys)[0]\n",
    "indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n",
    "sparse_labels = tf.cast(sparse_labels, tf.int32)\n",
    "concated = tf.concat(1, [indices, sparse_labels])\n",
    "outshape = tf.concat(0, [tf.reshape(derived_size, [1]), tf.reshape(num_labels, [1])])\n",
    "labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n",
    "print(labels.get_shape())\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys=np.asarray(ys)\n",
    "yzM=[];\n",
    "import math\n",
    "for r in range(4300):\n",
    "    ys[r]= math.ceil(ys[r])\n",
    "for r in range(ys.shape[0]):\n",
    "    i=ys[r];\n",
    "    yz=tf.zeros([1,10], tf.int32)\n",
    "    yz[i]=1;\n",
    "print(yz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.linspace(0.0, 128.0**2, 100)\n",
    "loss = error**2.0\n",
    "plt.plot(error, loss)\n",
    "plt.xlabel('error')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.linspace(0.0, 1.0, 100)\n",
    "plt.plot(error, error**2, label='l_2 loss')\n",
    "plt.plot(error, np.abs(error), label='l_1 loss')\n",
    "plt.xlabel('error')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first compute the error, the inner part of the summation.\n",
    "# This should be the l1-norm or l2-norm of the distance\n",
    "# between each color channel.\n",
    "error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_pred, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(Y_pred, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# %% We'll train in minibatches and report accuracy:\n",
    "batch_size = 100\n",
    "n_epochs = 100\n",
    "with tf.Session() as sess:\n",
    "    # Here we tell tensorflow that we want to initialize all\n",
    "    # the variables in the graph so we can use them\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Fit all training data\n",
    "    \n",
    "\n",
    "    prev_training_cost = 0.0\n",
    "    for epoch_i in range(n_epochs):\n",
    "        for (x, y) in zip(x_train, y_train):\n",
    "            sess.run(optimizer, feed_dict={X: xs, Y: labels})\n",
    "\n",
    "        training_cost = sess.run(\n",
    "            cost, feed_dict={X: x_train, Y: y_train})\n",
    "        print(training_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, compute the cost, as the mean error of the batch.\n",
    "# This should be a single value.\n",
    "cost = ...\n",
    "assert(cost.get_shape().as_list() == [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize all your variables and run the operation with your session\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# Optimize over a few iterations, each time following the gradient\n",
    "# a little at a time\n",
    "imgs = []\n",
    "costs = []\n",
    "gif_step = n_iterations // 10\n",
    "step_i = 0\n",
    "\n",
    "for it_i in range(n_iterations):\n",
    "    \n",
    "    # Get a random sampling of the dataset\n",
    "    idxs = np.random.permutation(range(len(xs)))\n",
    "    \n",
    "    # The number of batches we have to iterate over\n",
    "    n_batches = len(idxs) // batch_size\n",
    "    \n",
    "    # Now iterate over our stochastic minibatches:\n",
    "    for batch_i in range(n_batches):\n",
    "         \n",
    "        # Get just minibatch amount of data\n",
    "        idxs_i = idxs[batch_i * batch_size: (batch_i + 1) * batch_size]\n",
    "\n",
    "        # And optimize, also returning the cost so we can monitor\n",
    "        # how our optimization is doing.\n",
    "        training_cost = sess.run(\n",
    "            [cost, optimizer],\n",
    "            feed_dict={X: xs[idxs_i], Y: ys[idxs_i]})[0]\n",
    "\n",
    "    # Also, every 20 iterations, we'll draw the prediction of our\n",
    "    # input xs, which should try to recreate our image!\n",
    "    if (it_i + 1) % gif_step == 0:\n",
    "        costs.append(training_cost / n_batches)\n",
    "        ys_pred = Y_pred.eval(feed_dict={X: xs}, session=sess)\n",
    "        img = np.clip(ys_pred.reshape(img.shape), 0, 1)\n",
    "        imgs.append(img)\n",
    "        # Plot the cost over time\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].plot(costs)\n",
    "        ax[0].set_xlabel('Iteration')\n",
    "        ax[0].set_ylabel('Cost')\n",
    "        ax[1].imshow(img)\n",
    "        fig.suptitle('Iteration {}'.format(it_i))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
